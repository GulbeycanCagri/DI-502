# DI-502
DI 502 Project Repository — RAG-based Chatbot for Answering Finance Questions

## Features

*   **FastAPI Backend**: A lightweight web framework to serve the application and handle API requests.
*   **Llama 3 8B Integration**: Uses a 4-bit quantized version of `meta-llama/Meta-Llama-3-8B-Instruct` for efficient text generation.
*   **Retrieval-Augmented Generation (RAG)**: Leverages `llama-index` to answer questions based on the content of user-uploaded documents.
*   **Simple HTML/JS Frontend**: A clean, single-page interface for interacting with the chatbot.

---
## Project Structure

The repository is organized into a simple structure for a self-contained web application.

```
.
├── backend/
│   ├── src/            # Backend source code
│   ├── uploads/        # Default location for file uploads
│   ├── main.py         # FastAPI application entrypoint
│   └── requirements.txt  # Backend Python dependencies
├── data/               # Persistent data (e.g., vector stores)
├── frontend/           # Frontend application (React, Vue, etc.)
├── templates/          # (Optional: e.g., for Jinja2 templates if used)
├── .dockerignore
├── .gitignore
├── backend.Dockerfile    # Docker build instructions for the backend
├── docker-compose.yaml   # Defines all services (backend, frontend, nginx)
├── frontend.Dockerfile   # Docker build instructions for the frontend
├── nginx.conf          # Nginx configuration for reverse proxy
└── README.md
```

---
## Getting Started

Follow these instructions to get the project running on your local machine.

### Prerequisites

You'll need the following software installed:
*   [Python 3.10+](https://www.python.org/downloads/)
*   [Git](https://git-scm.com/)
*   An NVIDIA GPU is highly recommended for running the model.

### Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/GulbeycanCagri/DI-502.git
    cd DI-502
    ```

2.  **Create and activate a Python virtual environment:**
    ```bash
    python3 -m venv .venv
    source .venv/bin/activate
    # On Windows, use: .venv\Scripts\activate
    ```

3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
    *Note: This step may take some time as it downloads several large libraries, including PyTorch.*

---
## Usage

1.  **Activate the virtual environment (if not already active):**
    ```bash
    source .venv/bin/activate
    ```

2.  **Run the Flask application:**
    ```bash
    python app.py
    ```
    The application will start, and the model will be loaded into memory. This may take a few moments.

3.  **Access the chatbot:**
    Open your web browser and navigate to `http://127.0.0.1:8000`.

    You can now upload a document (`.pdf`, `.txt`, `.md`), ask a question about it, and receive an answer generated by the Llama 3 model.

---
## API Endpoints

The Flask application provides the following endpoints:

| Method | Endpoint | Description |
| :--- | :--- | :--- |
| `GET` | `/` | Serves the main chat page (`index.html`). |
| `POST` | `/api/chat` | Receives a question and a document, and returns a RAG-generated answer. |

---
